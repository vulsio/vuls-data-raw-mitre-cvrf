{
	"title": "CVE-2023-29374",
	"cve": "CVE-2023-29374",
	"notes": [
		{
			"type": "Description",
			"text": "In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks that can execute arbitrary code via the Python exec method."
		},
		{
			"title": "Published",
			"type": "Other",
			"text": "2023-04-04"
		},
		{
			"title": "Modified",
			"type": "Other",
			"text": "2023-04-04"
		}
	],
	"references": [
		{
			"url": "https://github.com/hwchase17/langchain/issues/1026",
			"description": "MISC:https://github.com/hwchase17/langchain/issues/1026"
		},
		{
			"url": "https://github.com/hwchase17/langchain/issues/814",
			"description": "MISC:https://github.com/hwchase17/langchain/issues/814"
		},
		{
			"url": "https://github.com/hwchase17/langchain/pull/1119",
			"description": "MISC:https://github.com/hwchase17/langchain/pull/1119"
		},
		{
			"url": "https://twitter.com/rharang/status/1641899743608463365/photo/1",
			"description": "MISC:https://twitter.com/rharang/status/1641899743608463365/photo/1"
		}
	]
}
